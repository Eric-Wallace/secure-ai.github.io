
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- Here are the current paper tags:
1) tag-all (for all papers)
2) tag-privacy
3) tag-host-security
4) tag-network-security
5) tag-theoretical-foundations


-->
<html><head>
<title>Security for Artificial Intelligence</title>
<style type="text/css">
body {
    margin-top: 30px;
    margin-bottom: 30px;
    margin-left: 100px;
    margin-right: 100px;
}
p {
    margin-top: 0px;
    margin-bottom: 0px;
}

.caption {
    font-size: 34px;
    font-weight: normal;
    color: #000;
    font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    font-weight: bold;
    color: #990000;
}
.caption-3 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    font-weight: bold;
    color: #F00;
}

.caption-4 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    color: #990000;
}
.content {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    text-align: justify;
}

.title-small {
    font-size: 20px;
    font-family: Georgia, "Times New Roman", Times, serif;
    font-weight: bold;
    color: #F90;
}
.title-large {
    font-size: 28px;
    font-family: Georgia, "Times New Roman", Times, serif;
    font-weight: bold;
    color: #000;
}
.margin {
    font-size: 10px;
    line-height: 10px;
}
.margin-small {
    font-size: 5px;
    line-height: 5px;
}
.margin-large {
    font-size: 16px;
    line-height: 16px;
}
</style>
<script type="text/javascript" src="jquery.js"></script>     
<script type="text/javascript"> 

function displaypage(){
	$(".tag-all").hide();
	if(document.getElementById('tag-all-box').checked){
		$(".tag-all").show();
	}
	if(document.getElementById('tag-theoretical-foundations-box').checked){
		$(".tag-theoretical-foundations").show();
	}
	if(document.getElementById('tag-host-security-box').checked){
		$(".tag-host-security").show();
	}
	if(document.getElementById('tag-network-security-box').checked){
		$(".tag-network-security").show();
	}
	if(document.getElementById('tag-privacy-box').checked){
		$(".tag-privacy").show();
	}

}

$(document).ready(function() {    

$("#tag-all-box").click(function() {
	displaypage();
});   
$("#tag-theoretical-foundations-box").click(function() {
	displaypage();
});
$("#tag-host-security-box").click(function() {
	displaypage();
}); 
$("#tag-network-security-box").click(function() {
	displaypage();
}); 
$("#tag-privacy-box").click(function() {
	displaypage();
}); 

});             
</script>     
<meta content="text/html; charset=unicode" http-equiv="Content-Type">
<meta name="GENERATOR" content="MSHTML 9.00.8112.16443"></head>
<body><center><h1><span>Security for Artificial Intelligence</span></h1></center>

<center>
		[<a href="#statement">Research Statement</a>] [<a href ="#publications">Publications</a>] [<a href="#members">Members</a>]  
</center>

<h2 class="label"><a name="statement"><span >Research Statement</span></a></h2> 
As intelligent systems become pervasive, safeguarding their security and privacy is critical. However, recent research have demonstrated that machine learning systems, including state-of-the-art deep neural networks, can be easily fooled by an adversary. For example, it is easy to generate adversarial examples, which are close to the benign inputs but are misidentified by the machine learning models. Moreover, in our recent work, we have shown that such attacks can be successful even without access to the model internals, i.e., in a black-box setting. These attacks may cause severe outcomes: for example, the adversary can mislead the perceptual systems of autonomous vehicles to wrongly identify road signs, which can result in catastrophic traffic accidents. Therefore, such security issues hinder the application of machine learning to security-critical systems.

<br><br>

In AI security research, we aim at investigating into the vulnerability of automatic learning systems, and ultimately, developing robust defense strategies against such sophisticated adversarial manipulations in real-world applications.
 
<hr>

<h2 class="label"><a name="publications"><span >Recent Publications</span></a></h2>

<!--<form> 
  <div id="tags"> 
    <input type="radio" name="tags" checked id="tag-all-box"> 
    <label for="tag-all-box">All Publications</label> 
   
    <input type="radio" name="tags" id="tag-theoretical-foundations-box"> 
    <label for="tag-theoretical-foundations-box">Theoretical Foundations</label>
 
    <input type="radio" name="tags" id="tag-host-security-box">
    <label for="tag-host-security-box">Host Security</label> 
    
    <input type="radio" name="tags" id="tag-network-security-box"> 
    <label for="tag-network-security-box">Network Security</label> 
    
    <input type="radio" name="tags" id="tag-privacy-box"> 
    <label for="tag-privacy-box">Privacy</label> 
    
  </div> 
</form> -->



<table border="0" cellpadding="0" cellspacing="15" width="100%">
  <tbody><tr>
    <td width="200"><img src="imgs/stopsign.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><a href="https://arxiv.org/abs/1707.08945"><strong>Robust Physical-World Attacks on Machine Learning Models</strong></a></p>
      <p class="content">Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, Dawn Song.</p>
      <p class="content">August, 2017.</p>
     <p class="margin-small">&nbsp;</p>
      <p class="content">
        Press: <a href="https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms">IEEE Spectrum</a> | <a href="https://sg.news.yahoo.com/researchers-demonstrate-limits-driverless-car-technology-151138885.html">Yahoo News</a> | <a href="https://www.wired.com/story/security-news-august-5-2017">Wired</a> | <a href="https://www.engadget.com/2017/08/06/altered-street-signs-confuse-self-driving-cars/">Engagdet</a> | <a href="http://www.telegraph.co.uk/technology/2017/08/07/graffiti-road-signs-could-trick-driverless-cars-driving-dangerously/">Telegraph</a> | <a href="http://blog.caranddriver.com/researchers-find-a-malicious-way-to-meddle-with-autonomous-cars/">Car and Driver</a> | <a href="https://www.cnet.com/roadshow/news/it-is-surprisingly-easy-to-bamboozle-a-self-driving-car/">CNET</a> | <a href="https://www.digitaltrends.com/cars/self-driving-cars-confuse-stickers-signs/">Digital Trends</a> | <a href="https://www.scmagazine.com/subtle-manipulation-of-street-signs-can-fool-self-driving-cars-researchers-report/article/680146/">SCMagazine</a> | <a href="https://www.schneier.com/blog/archives/2017/08/confusing_self-.html">Schneier on Security</a> | <a href="https://arstechnica.com/cars/2017/09/hacking-street-signs-with-stickers-could-confuse-self-driving-cars/?amp=1">Ars Technica</a> | <a href="http://fortune.com/2017/09/02/researchers-show-how-simple-stickers-could-trick-self-driving-cars/">Fortune</a>
      </p>
      </tr>
</tbody></table>

<table border="0" cellpadding="0" cellspacing="15" width="100%">
  <tbody><tr>
    <td width="200"><img src="imgs/ensembleDefense.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><a href="https://arxiv.org/abs/1706.04701"><strong>Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong</strong></a></p>
      <p class="content">Warren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song.</p>
      <p class="content">USENIX Workshop on Offensive Technologies (WOOT). August, 2017.</p>
     <p class="margin-small">&nbsp;</p>
      </tr>
</tbody></table>

<table border="0" cellpadding="0" cellspacing="15" width="100%">
  <tbody><tr>
    <td width="200"><img src="imgs/transferableAdv.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><a href="https://arxiv.org/abs/1611.02770"><strong>Delving into Transferable Adversarial Examples and Black-box Attacks</strong></a></p>
      <p class="content">Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.</p>
      <p class="content">International Conference on Learning Representations (ICLR). April, 2017.</p>
     <p class="margin-small">&nbsp;</p>
      </tr>
</tbody></table>

<table border="0" cellpadding="0" cellspacing="15" width="100%">
  <tbody><tr>
    <td width="200"><img src="imgs/rlAdv.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><a href="https://arxiv.org/abs/1705.06452"><strong>Delving into adversarial attacks on deep policies</strong></a></p>
      <p class="content">Jernej Kos and Dawn Song.</p>
      <p class="content">ICLR Workshop. April, 2017.</p>
     <p class="margin-small">&nbsp;</p>
      </tr>
</tbody></table>

<table border="0" cellpadding="0" cellspacing="15" width="100%">
  <tbody><tr>
    <td width="200"><img src="imgs/GANAdv.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><a href="https://arxiv.org/abs/1702.06832"><strong>Adversarial examples for generative models</strong></a></p>
      <p class="content">Jernej Kos, Ian Fischer, Dawn Song.</p>
      <p class="content">February, 2017.</p>
     <p class="margin-small">&nbsp;</p>
      </tr>
</tbody></table>

<hr>



<h2 class="label"><a name="members"><span >Members</span></a></h2>

<ul>
 <li>
  <p class="content"><b>Faculty:</b> <a href="http://www.cs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
 </li><br>
  <li><p class="content"><b>Postdocs:</b></p><ul>
    <li><p class="content"><a href="https://people.eecs.berkeley.edu/~liuchang/">Chang Liu</a></p></li>
    <li><p class="content"><a href="http://www.crystal-boli.com/">Bo Li</a></p></li>
  </ul><br></li>

  <li><p class="content"><b>Ph.D. Students:</b></p>
  <ul>
    <li><p class="content">Warren He</p></li>
    <li><p class="content"><a href="https://people.eecs.berkeley.edu/~ricshin/">Richard Shin</a></p></li>
    <li><p class="content">Xinyun Chen</p></li>
  </ul>
  <br>
  </li>

  <li><p class="content"><b>Others:</b></p>
 <ul>
   <li><p class="content">Jernej Kos (NUS)</p> </li>
  </ul><br>
  </li>
</li>
</ul>
<br><br></body></html>
